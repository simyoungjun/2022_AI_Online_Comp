{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba2dce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.4.0\n",
      "GPU installed:  True\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "# import sklearn\n",
    "# print(\"sklearn version: \", sklearn.__version__)\n",
    "# assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(\"TF version: \", tf.__version__)\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# GPU test\n",
    "print(\"GPU installed: \",tf.test.is_built_with_gpu_support())\n",
    "\n",
    "# To prevent \"CUDNN_STATUS_ALLOC_FAILED\" error with GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8616a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (33600, 514), Test: (7820, 514)\n",
      "['out' 'in' 'normal' 'other' 'noise']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "# import scoring as scoring\n",
    "import pickle\n",
    "import gzip\n",
    "from pyarrow import csv\n",
    "import csv\n",
    "# train_path = \"D:/2022AIComp_data/train.csv\"\n",
    "# test_path = \"D:/2022AIComp_data/test.csv\"\n",
    "\n",
    "#---------------------- Load Train,Test DF\n",
    "train_pd = pd.read_csv(\"D:/2022AIComp_data/train.csv\")\n",
    "test_pd = pd.read_csv(\"D:/2022AIComp_data/test.csv\")\n",
    "\n",
    "print(\"Train: %s, Test: %s\" %(train_pd.shape, test_pd.shape))\n",
    "train_pd.head()\n",
    "print(train_pd.iloc[:,0].unique())\n",
    "# test_pd.head()\n",
    "\n",
    "def remake_data():\n",
    "    X = np.array(train_pd.iloc[:,1:])\n",
    "    y = train_pd.iloc[:,0].replace(['out','in', 'normal', 'other', 'noise'],[0,1,2,3,4])\n",
    "    # y_train = np.argmax(np.array(pd.get_dummies(train_pd.iloc[:,0])),axis=1)\n",
    "\n",
    "    submit_test = np.array(test_pd.iloc[:,1:])\n",
    "\n",
    "    # y_test = np.array(pd.get_dummies(test_pd.iloc[:,0]))\n",
    "    #data Normalize\n",
    "\n",
    "    return X,y,submit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce99fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# from sklearn import model_selection\n",
    "class PreProcessing():\n",
    "#     def __init__(self,**kwargs):\n",
    "\n",
    "\n",
    "    def data_shape(self,*data_li,local = None):\n",
    "        for data in data_li:\n",
    "            vnames = [name for name in globals() if globals()[name] is data ]\n",
    "            if len(vnames) != 0:\n",
    "                print(vnames[0],'.shape : ',data.shape)\n",
    "            else:\n",
    "                if local == None: local = locals()\n",
    "                vnames = [name for name in local if local[name] is data]\n",
    "                print(vnames[0],'.shape : ',data.shape)\n",
    "        print('\\n')\n",
    "\n",
    "    \n",
    "    def data_reshape(self, *args, **kwargs):\n",
    "        reshaped = []\n",
    "        for data in args:\n",
    "#             print(data.shape)\n",
    "            reshaped.append(data.reshape((-1,27,19,1)))\n",
    "        return reshaped\n",
    "    \n",
    "    \n",
    "    def split(self,X,y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state = 42, stratify = y)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def make_label_set(self, X,y,label):\n",
    "        X_sub=[]\n",
    "        y_sub=[]\n",
    "        for i in label:\n",
    "            X_sub.append(X[y==i])\n",
    "            y_sub.append(y[y==i])\n",
    "        \n",
    "\n",
    "        X=np.concatenate(tuple(X_sub),axis = 0)\n",
    "        y=np.concatenate(tuple(y_sub),axis = 0)\n",
    "        y=np.array(pd.get_dummies(y))\n",
    "        \n",
    "        return X,y\n",
    "    \n",
    "    def pipeline(self, **kwargs):\n",
    "        X=kwargs.get('X')\n",
    "        y=kwargs.get('y')\n",
    "        submit=kwargs.get('submit')\n",
    "        shape = kwargs.get('shape')\n",
    "        label = kwargs.get('label')\n",
    "        \n",
    "        X,y=self.make_label_set(X,y,label)\n",
    "        self.data_shape(X,y,local = locals())\n",
    "        \n",
    "        if shape is not None:\n",
    "#             print(shape)\n",
    "#             print(len(data))\n",
    "            X,submit = self.data_reshape(X,submit,shape = shape)\n",
    "#             print(len(res))\n",
    "            \n",
    "        X_train,X_test,y_train,y_test = self.split(X,y)\n",
    "        self.data_shape(X_train,X_test,y_train,y_test,submit,local = locals())\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.submit = submit\n",
    "        \n",
    "        return X_train,X_test,y_train,y_test,submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae51b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 뽑아서 normal x , in,out 만\n",
    "batch_size= 64\n",
    "shape = (-1,27,19,1)\n",
    "\n",
    "X,y,submit = remake_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35d7ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X .shape :  (3600, 513)\n",
      "y .shape :  (3600, 2)\n",
      "\n",
      "\n",
      "X_train .shape :  (2880, 27, 19, 1)\n",
      "X_test .shape :  (720, 27, 19, 1)\n",
      "y_train .shape :  (2880, 2)\n",
      "y_test .shape :  (720, 2)\n",
      "submit .shape :  (7820, 27, 19, 1)\n",
      "\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_16 (Batc (None, 27, 19, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 27, 19, 16)        416       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 27, 19, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 9, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 13, 9, 64)         25664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 13, 9, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 6, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 6, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 6, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 428,646\n",
      "Trainable params: 428,228\n",
      "Non-trainable params: 418\n",
      "_________________________________________________________________\n",
      "2022-06-20_00_55\n",
      "Epoch 1/100\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.8351 - accuracy: 0.5491 - val_loss: 0.6825 - val_accuracy: 0.5434\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.6264 - accuracy: 0.6171 - val_loss: 0.6786 - val_accuracy: 0.5677\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.5885 - accuracy: 0.6854 - val_loss: 0.6780 - val_accuracy: 0.5503\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.5555 - accuracy: 0.7084 - val_loss: 0.6734 - val_accuracy: 0.5764\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.5308 - accuracy: 0.7419 - val_loss: 0.6738 - val_accuracy: 0.5851\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.4956 - accuracy: 0.7699 - val_loss: 0.6614 - val_accuracy: 0.5799\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.4848 - accuracy: 0.7701 - val_loss: 0.6317 - val_accuracy: 0.6163\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.4657 - accuracy: 0.7829 - val_loss: 0.6014 - val_accuracy: 0.6562\n",
      "Epoch 9/100\n",
      "16/36 [============>.................] - ETA: 0s - loss: 0.4390 - accuracy: 0.7953"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 49>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m train_par \u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#     'model':model, 'X_train':X_train, 'y_train':y_train, 'X_test':X_test,\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#         'y_test':y_test, 'submit':submit,\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m64\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_weight\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m20\u001b[39m}\n\u001b[0;32m     48\u001b[0m T\u001b[38;5;241m=\u001b[39mTrain_model()\n\u001b[1;32m---> 49\u001b[0m \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mTrain_model.compile_fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_class_weight(y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_train,axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss,optimizer \u001b[38;5;241m=\u001b[39m optimizer,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 79\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m               \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-----test score : \u001b[39m\u001b[38;5;124m'\u001b[39m,model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test))\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_curve(history)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1105\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1103\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1105\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:454\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 454\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:296\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    294\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 296\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hook))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:316\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    313\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    314\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 316\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    319\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(callback, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_supports_tf_logs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m numpy_logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Only convert once.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1020\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1020\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1084\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1083\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1084\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:514\u001b[0m, in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    512\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m--> 514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:659\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    656\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 659\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    660\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:659\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    655\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    656\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 659\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    660\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:510\u001b[0m, in \u001b[0;36mto_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    509\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, ops\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 510\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    512\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:1071\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \n\u001b[0;32m   1050\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1071\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:1037\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1036\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1038\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(core\u001b[38;5;241m.\u001b[39m_status_to_exception(e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmessage), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3631441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Train_model(PreProcessing):\n",
    "    \n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.args = args,\n",
    "        self.kwargs = kwargs\n",
    "#         self.model = model\n",
    "        self.batch_size = self.get('batch_size',64)\n",
    "        self.epoch = self.get('epoch',100)\n",
    "        self.class_weight = self.get('class_weight',True)\n",
    "        self.loss = self.get('loss','categorical_crossentropy')\n",
    "        self.optimizer  = self.get('optimizer','sgd')\n",
    "        self.patience = self.get('patience',20)\n",
    "#         self.X_train = X_train\n",
    "#         self.y_train = y_train\n",
    "#         self.X_test = X_test\n",
    "#         self.y_test = y_test\n",
    "#         self.submit = submit\n",
    "        \n",
    "    def get(self, instance_name,default):\n",
    "        return self.kwargs.get(instance_name,default)\n",
    "    \n",
    "    def this_time(self):\n",
    "        c = datetime.now() ##모델 돌릴 때 시작 시간\n",
    "        n_time = c.strftime('%Y-%m-%d_%H_%M')\n",
    "        print(n_time)\n",
    "        return(n_time)\n",
    "    \n",
    "    def model_save_path(self):\n",
    "        self.n_time = self.this_time()\n",
    "        MODEL_SAVE_FOLDER_PATH = './model_flow/'  #모델 저장 경로\n",
    "        if not os.path.exists(MODEL_SAVE_FOLDER_PATH): \n",
    "            os.mkdir(MODEL_SAVE_FOLDER_PATH) \n",
    "        model_path = MODEL_SAVE_FOLDER_PATH + str(self.n_time)+'.hdf5'\n",
    "        return model_path\n",
    "        \n",
    "    def early_modelcheck(self, model_path):\n",
    "            es = EarlyStopping(monitor='val_loss', mode='min', \n",
    "                       verbose=1, patience=self.patience, restore_best_weights = True)\n",
    "\n",
    "            mc = ModelCheckpoint(model_path, monitor='val_loss',\n",
    "                         mode='min', save_best_only=True)\n",
    "            return es,mc\n",
    "        \n",
    "    def calculate_class_weight(self,y_train):\n",
    "        \n",
    "        \n",
    "#         classes_in = np.argmax(y_train,axis =1)\n",
    "#         print(classes_in)\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight = 'balanced',classes = np.unique(y_train),y = y_train)\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "        return class_weights\n",
    "    \n",
    "    #   model,X_train,y_train,X_test,y_test = args\n",
    "    def compile_fit(self, *args, **kwargs): \n",
    "        loss = kwargs.pop('loss',self.loss)\n",
    "        optimizer = kwargs.pop('optimizer',self.optimizer)\n",
    "        \n",
    "        label= self.get('label',None)\n",
    "        \n",
    "        model,X_train,y_train,X_test,y_test = args\n",
    "        \n",
    "        model_path = self.model_save_path()\n",
    "        es,mc = self.early_modelcheck(model_path)\n",
    "        \n",
    "        \n",
    "        class_weights = self.calculate_class_weight(y_train = np.argmax(y_train,axis =1))\n",
    "        \n",
    "        model.compile(loss=loss,optimizer = optimizer,metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(X_train,y_train, epochs = self.epoch,\n",
    "                     batch_size=self.batch_size, validation_split=0.2, \n",
    "                                callbacks=[es,mc],\n",
    "                               class_weight = class_weights,\n",
    "                                shuffle=True,\n",
    "                       ).history\n",
    "        \n",
    "        print('\\n-----test score : ',model.evaluate(X_test, y_test))\n",
    "        \n",
    "        self.learning_curve(history)\n",
    "        \n",
    "        y_pred = self.predict_label(model,X_test,y_test,label)\n",
    "        \n",
    "        return model,y_pred\n",
    "        \n",
    "    def learning_curve(self,history):\n",
    "    \n",
    "        #러닝 커브\n",
    "        plt.plot(history['accuracy'], label='train_acc')\n",
    "        plt.plot(history['val_accuracy'], label= 'val_acc')\n",
    "        plt.plot(history['loss'], label= 'train_loss')\n",
    "        plt.plot(history['val_loss'], label= 'val_loss')\n",
    "        plt.title('Learning Curve')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.show()\n",
    "    \n",
    "    # model, X_train,y_train,X_test,y_test =args\n",
    "    def train_pipeline(self,*args,**kwargs):\n",
    "        model = self.compile_fit(self,*args,loss=self.loss,\n",
    "                                 optimizer = self.optimizer)\n",
    "        \n",
    "        y_pred = self.predict_label(model,*args,**kargs)\n",
    "        \n",
    "    \n",
    "    #\n",
    "    def predict_label(self,model,X_test,y_test,label=None):\n",
    "        \n",
    "        self.data_shape(X_test,y_test,local=locals())\n",
    "        y_pred = model.predict(X_test)\n",
    "        if label != None:\n",
    "            y_pred_re = real_pred(y_pred,label)\n",
    "        else:\n",
    "            y_pred_re =y_pred\n",
    "        self.show_matrix(y_test,y_pred)\n",
    "\n",
    "        return y_pred_re\n",
    "    \n",
    "    def real_pred(y_pred,label):\n",
    "        for la_be, la_af in enumerate(label):\n",
    "            y_pred[y_pred==la_be]=la_af\n",
    "        return y_pred\n",
    "    \n",
    "    def show_matrix(self,y_test,y_pred):\n",
    "        \n",
    "        y_pred = np.argmax(y_pred,axis =1)\n",
    "        if len(y_test[0])!=1:\n",
    "            y_test = np.argmax(y_test,axis =1)\n",
    "\n",
    "        cf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        per_cf =[]\n",
    "        for i in cf:\n",
    "            per_cf.append(i/np.sum(i))\n",
    "        axes=[]\n",
    "    #     plt.rc('font', size=10)\n",
    "        for i , cf in enumerate([cf, per_cf]):\n",
    "            fig=plt.figure(figsize=(8,8))\n",
    "            axes.append(fig.add_subplot(2,2,i+1))\n",
    "            ax = sns.heatmap(np.round(cf,4),annot=True, fmt='', cmap='Blues')#확률로 표시\n",
    "            ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "            ax.set_xlabel('\\nPredicted Values')\n",
    "            ax.set_ylabel('Actual Values ');\n",
    "            fig.tight_layout()\n",
    "            ## Display the visualiztion of the Confusion Matrix.\n",
    "            plt.show()\n",
    "        print('model_name : ', self.n_time)\n",
    "    \n",
    "    \n",
    "    def pred_sum(self,*args, **kwargs):\n",
    "        model= kwargs.pop('model',self.model)\n",
    "        \n",
    "    \n",
    "    def make_test_csv(self, model,submit_test,model_name,reshape=False):\n",
    "        sample_pd = pd.read_csv(\"D:/2022AIComp_data/sample_submission.csv\")\n",
    "        sample_pd.set_index('id',inplace=True)\n",
    "\n",
    "        if reshape == True:\n",
    "            submit_te = submit_test.reshape((-1,27,19,1))\n",
    "        else:\n",
    "            submit_te=submit_test\n",
    "        pred = np.argmax(model.predict(submit_te),axis = 1)\n",
    "        print(pred.shape)\n",
    "        sub = pd.concat([test_pd.iloc[:,0],\n",
    "                               pd.DataFrame(pred,columns=['leaktype']).replace([0,1,2,3,4],['out','in', 'normal', 'other', 'noise'])],axis=1)\n",
    "\n",
    "        sub.set_index('id',inplace=True)\n",
    "        submit_pd = sub.reindex(sample_pd.index)\n",
    "        print('제출 할 csv 클래스 별 분포 : ', submit_pd.value_counts())\n",
    "        plt.hist(np.array(submit_pd))\n",
    "        plt.show()\n",
    "\n",
    "        MODEL_SAVE_FOLDER_PATH = './result_csv/' \n",
    "        if not os.path.exists(MODEL_SAVE_FOLDER_PATH): \n",
    "            os.mkdir(MODEL_SAVE_FOLDER_PATH) \n",
    "        submit_pd.to_csv(MODEL_SAVE_FOLDER_PATH + model_name[:-5]+'.csv')\n",
    "        print(model_name)\n",
    "        return submit_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de28fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X .shape :  (3600, 513)\n",
      "y .shape :  (3600, 2)\n",
      "\n",
      "\n",
      "X_train .shape :  (2880, 27, 19, 1)\n",
      "X_test .shape :  (720, 27, 19, 1)\n",
      "y_train .shape :  (2880, 2)\n",
      "y_test .shape :  (720, 2)\n",
      "submit .shape :  (7820, 27, 19, 1)\n",
      "\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_20 (Batc (None, 27, 19, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 27, 19, 16)        416       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 27, 19, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 13, 9, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 13, 9, 64)         25664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 13, 9, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 6, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 6, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 6, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 428,646\n",
      "Trainable params: 428,228\n",
      "Non-trainable params: 418\n",
      "_________________________________________________________________\n",
      "2022-06-20_00_56\n",
      "Epoch 1/100\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 1.0632 - accuracy: 0.5475 - val_loss: 0.6820 - val_accuracy: 0.5538\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.6157 - accuracy: 0.6485 - val_loss: 0.6761 - val_accuracy: 0.5972\n",
      "Epoch 3/100\n",
      " 9/36 [======>.......................] - ETA: 0s - loss: 0.5726 - accuracy: 0.6918"
     ]
    }
   ],
   "source": [
    "#feature 뽑아서 normal x , in,out 만\n",
    "batch_size= 64\n",
    "shape = (-1,27,19,1)\n",
    "label = [0,1]\n",
    "out_num = len(label)\n",
    "X,y,submit = remake_data()\n",
    "# X=np.concatenate((X[:3600,1:140],X[:3600,240:321],X[:3600,400:500]),axis = 1)\n",
    "# submit_test=np.concatenate((submit_test[:,1:140],submit_test[:,240:321],submit_test[:,400:500]),axis = 1)\n",
    "# X=X[-12000:]\n",
    "# y= y[-12000:]\n",
    "# print(X.reshape(shape).shape)\n",
    "\n",
    "\n",
    "# y_train = ohe.fit_transform(y_train.values.reshape(-1,1))\n",
    "prep = {'X' : X, 'y': y, 'submit': submit,'shape' : shape,'label':label }\n",
    "p=PreProcessing()\n",
    "X_train,X_test,y_train,y_test,submit = p.pipeline(**prep)\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (X_train.shape[1],X_train.shape[2],1)\n",
    "model = keras.models.Sequential([\n",
    "    keras.Input(shape=input_shape),\n",
    "    layers.BatchNormalization(),\n",
    "#     keras.layers.experimental.preprocessing.Resizing(32,32),\n",
    "    keras.layers.Conv2D(16, kernel_size=5, strides=1,  activation='relu', padding='same'), #C11\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(),\n",
    "    keras.layers.Conv2D(64, kernel_size=5, strides=1, activation='relu', padding='same'), #C3\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(),\n",
    "    keras.layers.Conv2D(128, kernel_size=5, strides=1, activation='relu', padding='same'), #C5\n",
    "    layers.BatchNormalization(),\n",
    "    keras.layers.Flatten(), #Flatten    \n",
    "    keras.layers.Dense(64, activation='relu'), #F6\n",
    "    keras.layers.Dense(out_num, activation='softmax') #Output layer\n",
    "    ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "train_par ={\n",
    "#     'model':model, 'X_train':X_train, 'y_train':y_train, 'X_test':X_test,\n",
    "#         'y_test':y_test, 'submit':submit,\n",
    "        'batch_size':64, 'optimizer':'sgd', 'epoch':100, 'class_weight':True,\n",
    "        'loss': 'categorical_crossentropy','patience':20}\n",
    "\n",
    "T=Train_model()\n",
    "T.compile_fit(model, X_train,y_train,X_test,y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# y_pred, submit = train_model(model,data,\n",
    "#             batch_size=batch_size,patience = 20,optimizer = 'sgd',reshape=False,\n",
    "#             class_weight=False,label_encoder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af724a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6796e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fe698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4102cdac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2022AIComp",
   "language": "python",
   "name": "2022aicomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
